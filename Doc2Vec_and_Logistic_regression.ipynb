{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm0005211</td>\n",
       "      <td>Danica McKellar</td>\n",
       "      <td>danica mckellarmckellar 2018u. . nation book f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm0005576</td>\n",
       "      <td>Drea de Matteo</td>\n",
       "      <td>drea de matteod matteo 2005bornandrea donna de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0028846</td>\n",
       "      <td>Shawn Andrews</td>\n",
       "      <td>american footbal guard tackl actor , see shawn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0036571</td>\n",
       "      <td>Monica</td>\n",
       "      <td>look monica monica wiktionari , free dictionar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm0038875</td>\n",
       "      <td>John Asher</td>\n",
       "      <td>thi biographi live person need addit citat ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actor_id       actor_name  \\\n",
       "0  nm0005211  Danica McKellar   \n",
       "1  nm0005576   Drea de Matteo   \n",
       "2  nm0028846    Shawn Andrews   \n",
       "3  nm0036571           Monica   \n",
       "4  nm0038875       John Asher   \n",
       "\n",
       "                                               words  \n",
       "0  danica mckellarmckellar 2018u. . nation book f...  \n",
       "1  drea de matteod matteo 2005bornandrea donna de...  \n",
       "2  american footbal guard tackl actor , see shawn...  \n",
       "3  look monica monica wiktionari , free dictionar...  \n",
       "4  thi biographi live person need addit citat ver...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_data import load_data\n",
    "data1_path = \"../data/actor_words.csv\"\n",
    "data2_path = \"../data/actors_70s.csv\"\n",
    "df_1000p = load_data(data1_path, data2_path)\n",
    "df_1000p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_cluster_datasets = df_1000p.copy()\n",
    "# Limit the max_features \n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_8 = vectorizer.fit_transform(df_1000p['words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# clusters = default 8\n",
    "kmeans_n8 = KMeans()\n",
    "kmeans_n8.fit(X_8)\n",
    "top_centroids = kmeans_n8.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "assigned_cluster = kmeans_n8.transform(X_8).argmin(axis=1)\n",
    "assigned_cluster_datasets['assigned_cluster'] = assigned_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>words</th>\n",
       "      <th>assigned_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nm0005211</td>\n",
       "      <td>Danica McKellar</td>\n",
       "      <td>danica mckellarmckellar 2018u. . nation book f...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nm0005576</td>\n",
       "      <td>Drea de Matteo</td>\n",
       "      <td>drea de matteod matteo 2005bornandrea donna de...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nm0028846</td>\n",
       "      <td>Shawn Andrews</td>\n",
       "      <td>american footbal guard tackl actor , see shawn...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nm0036571</td>\n",
       "      <td>Monica</td>\n",
       "      <td>look monica monica wiktionari , free dictionar...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nm0038875</td>\n",
       "      <td>John Asher</td>\n",
       "      <td>thi biographi live person need addit citat ver...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actor_id       actor_name  \\\n",
       "0  nm0005211  Danica McKellar   \n",
       "1  nm0005576   Drea de Matteo   \n",
       "2  nm0028846    Shawn Andrews   \n",
       "3  nm0036571           Monica   \n",
       "4  nm0038875       John Asher   \n",
       "\n",
       "                                               words  assigned_cluster  \n",
       "0  danica mckellarmckellar 2018u. . nation book f...                 7  \n",
       "1  drea de matteod matteo 2005bornandrea donna de...                 4  \n",
       "2  american footbal guard tackl actor , see shawn...                 3  \n",
       "3  look monica monica wiktionari , free dictionar...                 5  \n",
       "4  thi biographi live person need addit citat ver...                 6  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigned_cluster_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(assigned_cluster_datasets, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "#     tokens = []\n",
    "#     for sent in nltk.sent_tokenize(text):\n",
    "#         for word in nltk.word_tokenize(sent):\n",
    "#             if len(word) < 2:\n",
    "#                 continue\n",
    "#             tokens.append(word.lower())\n",
    "    return text\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['words']), tags=[r.assigned_cluster]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['words']), tags=[r.assigned_cluster]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 83764.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 223457.86it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 889700.85it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 884874.26it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 911239.23it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 710898.98it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 781270.04it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 422204.89it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1005828.30it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 817830.86it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 840782.59it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 833147.79it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1008592.51it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 865825.07it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 930295.56it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1075068.77it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1003422.01it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 938623.02it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1031265.47it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 964841.54it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 507609.41it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 954490.51it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 896218.80it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1112547.48it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 786923.83it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 922984.22it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 754759.07it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 950165.95it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 747267.19it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 888623.73it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 966747.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words.split(' '), steps=20)) for doc in sents])\n",
    "    return targets, regressors \n",
    "\n",
    "# def vec_for_learning(model, tagged_docs):\n",
    "#     sents = tagged_docs.values\n",
    "#     targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "#     return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.35\n",
      "Testing F1 score: 0.3383414976955238\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 92519.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dmm.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:00<00:00, 272914.37it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 926771.72it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 238138.76it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 614743.05it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 849295.00it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 540702.17it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 963574.93it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1069196.21it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1119333.89it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 878256.90it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 714706.13it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1072712.02it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 645419.39it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 347169.54it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 847334.14it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1091048.98it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1000686.03it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 572992.35it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 542199.96it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 915501.34it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 762600.73it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 986232.05it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 997964.92it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1090643.68it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1207738.71it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 638541.28it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 917790.81it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 969941.46it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 912088.47it/s]\n",
      "100%|██████████| 700/700 [00:00<00:00, 1111284.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dmm.alpha -= 0.002\n",
    "    model_dmm.min_alpha = model_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.6233333333333333\n",
      "Testing F1 score: 0.6090011593217127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = vec_for_learning(model_dmm, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dmm, test_tagged)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words.split(\" \"), steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy 0.5733333333333334\n",
      "Testing F1 score: 0.5711116084140309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train, X_train = get_vectors(new_model, train_tagged)\n",
    "y_test, X_test = get_vectors(new_model, test_tagged)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
